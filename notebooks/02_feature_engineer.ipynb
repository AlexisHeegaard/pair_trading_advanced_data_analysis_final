{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1791d946",
   "metadata": {},
   "source": [
    "this notebook generates the imput features on which our models will learn\n",
    "\n",
    "we start with the 2 more obvious features, which are Z-Score (metric that standardize spread relative to its recent mean and volatility) and spread volatlity\n",
    "\n",
    "More possible feature inputs are computed, organized into 3 groups:\n",
    "-standardized deviation metrics\n",
    "-Oscillator and postioning metrics\n",
    "-volatility and dynamics metrics\n",
    "\n",
    "\n",
    "All features values are computed for each gold tier pairs choosen in the previous stage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee93f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../data/processed/03_gold_tier_pairs.csv...\n",
      "\n",
      "--- Processing Sector: financials (3 pairs) ---\n",
      "\n",
      "--- Processing Sector: industrials (1 pairs) ---\n",
      "\n",
      "--- Processing Sector: utilities (1 pairs) ---\n",
      "\n",
      "‚úÖ SUCCESS! Generated dataset with 5 pairs.\n",
      "Total Rows: 5868\n",
      "\n",
      "üìä Features (10 total):\n",
      "    1. Z_Score\n",
      "    2. Volatility\n",
      "    3. Extreme_Z\n",
      "    4. Distance_From_Mean\n",
      "    5. Spread_Normalized\n",
      "    6. Range_Position\n",
      "    7. Recent_Extreme\n",
      "    8. MR_Strength\n",
      "    9. Vol_Expansion\n",
      "   10. Speed_To_Mean\n"
     ]
    }
   ],
   "source": [
    "#TAKE GOLD TIER PAIRS AND CONVERT THEIR DATA INTO ML-READY FEATURES\n",
    "\n",
    "# create many features an test their correlation to the target \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "raw_dir = '../data/raw'\n",
    "gold_pairs_path = '../data/processed/03_gold_tier_pairs.csv' \n",
    "output_path = '../data/processed/04_ml_ready_features.csv'\n",
    "\n",
    "print(f\"Loading {gold_pairs_path}...\")\n",
    "gold_pairs = pd.read_csv(gold_pairs_path)\n",
    "\n",
    "all_pairs_data = []\n",
    "grouped = gold_pairs.groupby('Sector')\n",
    "\n",
    "for sector_name, group in grouped:\n",
    "    print(f\"\\n--- Processing Sector: {sector_name} ({len(group)} pairs) ---\")\n",
    "    \n",
    "    price_path = os.path.join(raw_dir, f\"{sector_name}_prices.csv\")\n",
    "    try:\n",
    "        prices = pd.read_csv(price_path, index_col=0, parse_dates=True)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"   Missing price file for {sector_name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        s1, s2 = row['Stock1'], row['Stock2']\n",
    "        pair_name = f\"{s1}-{s2}\"\n",
    "        \n",
    "        try:\n",
    "            df_pair = prices[[s1, s2]].dropna()\n",
    "            hedge_ratio = row['Hedge_Ratio']\n",
    "            spread = df_pair[s1] - (hedge_ratio * df_pair[s2])\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        df = pd.DataFrame(index=spread.index)\n",
    "        df['Spread'] = spread\n",
    "        \n",
    "        window = 20\n",
    "        \n",
    "        # MEAN REVERSION FOCUSED FEATURES\n",
    "        \n",
    "        rolling_mean = spread.rolling(window).mean()\n",
    "        rolling_std = spread.rolling(window).std()\n",
    "        rolling_min = spread.rolling(window).min()\n",
    "        rolling_max = spread.rolling(window).max()\n",
    "        \n",
    "        # 1. Z_Score (original feature for the model, standadrized deviation metric)\n",
    "        df['Z_Score'] = (spread - rolling_mean) / rolling_std\n",
    "        \n",
    "        # 2. Volatility (original feature for the model)\n",
    "        df['Volatility'] = rolling_std\n",
    "        \n",
    "#ADDITIONAL FEATURE FOR MODEL PRECISION ENHANCEMENTS\n",
    "\n",
    "    ##standarddized deviation metrics\n",
    "\n",
    "        # 3. EXTREME Z_SCORE INDICATOR (binary: is spread at extreme?)\n",
    "        # High Z correlates with reversion if mean reversion works\n",
    "        df['Extreme_Z'] = ((df['Z_Score'].abs() > 1.5).astype(int))\n",
    "        \n",
    "        # 4. DISTANCE FROM MEAN (absolute deviation)\n",
    "        # How far from equilibrium? Bigger distance = stronger reversion pressure\n",
    "        df['Distance_From_Mean'] = (spread - rolling_mean).abs() / (rolling_std + 1e-6)\n",
    "        \n",
    "        # 5. SPREAD ITSELF (normalized)\n",
    "        # Sometimes raw spread level matters\n",
    "        df['Spread_Normalized'] = spread / (rolling_std + 1e-6)\n",
    "\n",
    "\n",
    "    ##Oscillator & positioning metrics\n",
    "\n",
    "        # 6. RECENT RANGE POSITION (where in recent min/max?)\n",
    "        # Low = near recent low (might bounce up)\n",
    "        # High = near recent high (might bounce down)\n",
    "        df['Range_Position'] = (spread - rolling_min) / (rolling_max - rolling_min + 1e-6)\n",
    "\n",
    "        # 7. EXTREME BOUNCE INDICATOR\n",
    "        # Was spread recently extreme? (strong predictor of reversal)\n",
    "        was_extreme = ((spread.shift(1).abs() > rolling_std.shift(1) * 2).astype(int))\n",
    "        df['Recent_Extreme'] = was_extreme\n",
    "        \n",
    "\n",
    "    ##volatility & dynamics metrics\n",
    "\n",
    "        # 5. MEAN REVERSION SPEED (how fast should it revert?)\n",
    "        # Distance * Z_Score direction = strength & direction of reversion\n",
    "        df['MR_Strength'] = np.sign(rolling_mean - spread) * ((spread - rolling_mean).abs() / (rolling_std + 1e-6))\n",
    "        \n",
    "        \n",
    "        # 8. VOLATILITY EXPANSION (is vol increasing?)\n",
    "        # High vol might suppress mean reversion\n",
    "        vol_sma = rolling_std.rolling(10).mean()\n",
    "        df['Vol_Expansion'] = rolling_std / (vol_sma + 1e-6)\n",
    "        \n",
    "        # 9. MEAN REVERSION VELOCITY (speed of change toward mean)\n",
    "        # How fast is spread moving toward mean?\n",
    "        days_to_mean_at_current_speed = (spread - rolling_mean) / (spread.diff() + 1e-6)\n",
    "        df['Speed_To_Mean'] = days_to_mean_at_current_speed.rolling(5).mean()\n",
    "        \n",
    "        \n",
    "        # TARGET OUTPUT GENERATION \n",
    "        df['Target_Return'] = spread.shift(-10) - spread\n",
    "        df['Target_Direction'] = (df['Target_Return'] > 0).astype(int)\n",
    "        \n",
    "        # Metadata\n",
    "        df['Pair_ID'] = pair_name\n",
    "        df['Sector'] = sector_name\n",
    "        \n",
    "        all_pairs_data.append(df.dropna())\n",
    "\n",
    "# COMBINE & SAVE\n",
    "if all_pairs_data:\n",
    "    ml_dataset = pd.concat(all_pairs_data)\n",
    "    ml_dataset.to_csv(output_path)\n",
    "    \n",
    "    print(f\"\\n‚úÖ SUCCESS! Generated dataset with {ml_dataset['Pair_ID'].nunique()} pairs.\")\n",
    "    print(f\"Total Rows: {len(ml_dataset)}\")\n",
    "    \n",
    "    # Show all generated features\n",
    "    feature_cols = [col for col in ml_dataset.columns \n",
    "                   if col not in ['Spread', 'Target_Return', 'Target_Direction', 'Pair_ID', 'Sector']]\n",
    "    print(f\"\\nüìä Features ({len(feature_cols)} total):\")\n",
    "    for i, col in enumerate(feature_cols, 1):\n",
    "        print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No pairs found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada41b6d",
   "metadata": {},
   "source": [
    "these metrics relevance to output target are tested using the pearson correlation factor\n",
    "Only feature with significant correlation will be kept as input to feed the leaning models\n",
    "\n",
    "Since we are trying to predict binary values with LSTM and Ridge Classifier, compute correlation  between target direction and features, instead of target return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2645b2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target_Direction      1.000000\n",
      "Target_Return         0.635097\n",
      "MR_Strength           0.138193\n",
      "Vol_Expansion         0.012738\n",
      "Speed_To_Mean         0.001621\n",
      "Spread_Normalized     0.000190\n",
      "Distance_From_Mean   -0.013124\n",
      "Volatility           -0.014182\n",
      "Recent_Extreme       -0.021957\n",
      "Spread               -0.023312\n",
      "Extreme_Z            -0.025408\n",
      "Z_Score              -0.138193\n",
      "Range_Position       -0.151624\n",
      "Name: Target_Direction, dtype: float64\n",
      "\n",
      " Feature Correlation Table:\n",
      "               Feature  Correlation  Abs_Correlation\n",
      "10    Target_Direction     1.000000         1.000000\n",
      "5       Range_Position    -0.151624         0.151624\n",
      "7          MR_Strength     0.138193         0.138193\n",
      "0              Z_Score    -0.138193         0.138193\n",
      "2            Extreme_Z    -0.025408         0.025408\n",
      "6       Recent_Extreme    -0.021957         0.021957\n",
      "1           Volatility    -0.014182         0.014182\n",
      "3   Distance_From_Mean    -0.013124         0.013124\n",
      "8        Vol_Expansion     0.012738         0.012738\n",
      "9        Speed_To_Mean     0.001621         0.001621\n",
      "4    Spread_Normalized     0.000190         0.000190\n",
      "\n",
      " Table saved to: ../data/processed/05_feature_correlations.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/processed/04_ml_ready_features.csv', index_col=0, parse_dates=True)\n",
    "print(df.drop(['Pair_ID', 'Sector'], axis=1).corr()['Target_Direction'].sort_values(ascending=False))\n",
    "\n",
    "correlations = df.drop(['Pair_ID', 'Sector', 'Target_Return', 'Spread'], axis=1).corr()['Target_Direction']\n",
    "\n",
    "results_table = pd.DataFrame({\n",
    "    'Feature': correlations.index,\n",
    "    'Correlation': correlations.values,\n",
    "    'Abs_Correlation': correlations.abs().values  \n",
    "})\n",
    "\n",
    "results_table = results_table.sort_values(by='Abs_Correlation', ascending=False)\n",
    "\n",
    "print(\"\\n Feature Correlation Table:\")\n",
    "print(results_table)\n",
    "\n",
    "# 8. Save to CSV for your report\n",
    "results_table.to_csv('../data/processed/05_feature_correlations.csv', index=False)\n",
    "print(\"\\n Table saved to: ../data/processed/05_feature_correlations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c424283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table successfully saved to: ../results\\feature_correlations.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the paths\n",
    "source_path = '../data/processed/05_feature_correlations.csv'\n",
    "output_dir = '../results'\n",
    "output_path = os.path.join(output_dir, 'feature_correlations.csv')\n",
    "\n",
    "# 2. Load the CSV \n",
    "df_final = pd.read_csv(source_path)\n",
    "\n",
    "# 4. Save the table to the output folder\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Table successfully saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c938a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 5868\n",
      "Unique pairs: 5\n",
      "Rows per pair:\n",
      "  FITB-PNC: 1146\n",
      "  AIG-CB: 1146\n",
      "  MS-STT: 1146\n",
      "  AME-ITW: 1215\n",
      "  CMS-DUK: 1215\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/processed/04_ml_ready_features.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Unique pairs: {df['Pair_ID'].nunique()}\")\n",
    "print(f\"Rows per pair:\")\n",
    "for pair in df['Pair_ID'].unique():\n",
    "    n = (df['Pair_ID'] == pair).sum()\n",
    "    print(f\"  {pair}: {n}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
