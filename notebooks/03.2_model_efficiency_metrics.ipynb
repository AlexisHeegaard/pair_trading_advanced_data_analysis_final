{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "307cabe3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72062dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading model predictions...\n",
      "   Loaded 2405 rows.\n",
      "   Date Range: 2024-01-17 to 2025-12-15\n",
      "   Pairs: 5 unique pairs\n",
      "GLOBAL MODEL PERFORMANCE\n",
      "       Accuracy  Precision  Recall  F1_Score  ROC_AUC\n",
      "Model                                                \n",
      "Ridge    0.5543     0.5706  0.5670    0.5688   0.5538\n",
      "LSTM     0.5638     0.5650  0.6905    0.6214   0.5803\n",
      "PREFERRED MODEL: Ridge (Highest Precision)\n",
      "PERFORMANCE BY PAIR\n",
      "Which pairs the models are actually good at predicting?\n",
      "\n",
      "TOP 5 PAIRS (by LSTM Precision):\n",
      "          Samples  Ridge_Prec  LSTM_Prec\n",
      "Pair_ID                                 \n",
      "AME-ITW       481      0.5869     0.6139\n",
      "CMS-DUK       481      0.5894     0.5922\n",
      "MS-STT        481      0.5801     0.5647\n",
      "AIG-CB        481      0.5681     0.5426\n",
      "FITB-PNC      481      0.5345     0.5263\n",
      "\n",
      "BOTTOM 5 PAIRS (Avoid these):\n",
      "          Samples  Ridge_Prec  LSTM_Prec\n",
      "Pair_ID                                 \n",
      "AME-ITW       481      0.5869     0.6139\n",
      "CMS-DUK       481      0.5894     0.5922\n",
      "MS-STT        481      0.5801     0.5647\n",
      "AIG-CB        481      0.5681     0.5426\n",
      "FITB-PNC      481      0.5345     0.5263\n",
      "MODEL CONSENSUS ANALYSIS\n",
      "Strategy: Trade only when BOTH models agree (Converge)\n",
      "   - Trades Triggered: 1131 / 2405\n",
      "   - Hybrid Accuracy:  0.5551\n",
      "   - Hybrid Precision: 0.5782\n",
      "\n",
      " INSIGHT: The Hybrid consensus improves Precision! Use this for safer trades.\n",
      "Saved global metrics to: ../data/processed/06_global_metrics.csv\n",
      "Saved pair performance to: ../data/processed/06_pair_performance.csv\n",
      "\n",
      " Evaluation Complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, roc_auc_score, confusion_matrix\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# 1. LOAD PREDICTIONS\n",
    "print(\" Loading model predictions...\")\n",
    "file_path = '../data/processed/05_model_predictions.csv'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\" Error: File not found at {file_path}\")\n",
    "    print(\"   Run '05_Model_Prediction.ipynb' first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "df = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "print(f\"   Loaded {len(df)} rows.\")\n",
    "print(f\"   Date Range: {df.index.min().date()} to {df.index.max().date()}\")\n",
    "print(f\"   Pairs: {df['Pair_ID'].nunique()} unique pairs\")\n",
    "\n",
    "\n",
    "# 2. GLOBAL METRICS\n",
    "\n",
    "print(\"GLOBAL MODEL PERFORMANCE\")\n",
    "\n",
    "\n",
    "y_true = df['Target_Direction'].values\n",
    "# Ensure binary targets\n",
    "y_true_binary = (y_true > 0.5).astype(int)\n",
    "\n",
    "models = ['Ridge', 'LSTM']\n",
    "metrics_list = []\n",
    "\n",
    "for model in models:\n",
    "    # Get columns dynamically\n",
    "    pred_col = f\"{model}_Pred\"\n",
    "    prob_col = f\"{model}_Prob\" if f\"{model}_Prob\" in df.columns else pred_col\n",
    "    \n",
    "    y_pred = df[pred_col].values\n",
    "    y_prob = df[prob_col].values\n",
    "    \n",
    "    # Binary conversion if needed\n",
    "    y_pred_bin = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate Core Metrics\n",
    "    acc = accuracy_score(y_true_binary, y_pred_bin)\n",
    "    prec = precision_score(y_true_binary, y_pred_bin, zero_division=0)\n",
    "    rec = recall_score(y_true_binary, y_pred_bin, zero_division=0)\n",
    "    f1 = f1_score(y_true_binary, y_pred_bin, zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_true_binary, y_prob)\n",
    "    except:\n",
    "        auc = 0.5\n",
    "        \n",
    "    metrics_list.append({\n",
    "        'Model': model,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1_Score': f1,\n",
    "        'ROC_AUC': auc\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list).set_index('Model')\n",
    "print(metrics_df.round(4).to_string())\n",
    "\n",
    "\n",
    "# Identify Winner\n",
    "winner = metrics_df['Precision'].idxmax() # Precision is king in trading\n",
    "print(f\"PREFERRED MODEL: {winner} (Highest Precision)\")\n",
    "\n",
    "\n",
    "# 3. PER-PAIR BREAKDOWN (Critical for later Trading backtest)\n",
    "\n",
    "\n",
    "print(\"PERFORMANCE BY PAIR\")\n",
    "print(\"Which pairs the models are actually good at predicting?\\n\")\n",
    "\n",
    "pair_stats = []\n",
    "\n",
    "for pair in df['Pair_ID'].unique():\n",
    "    pair_data = df[df['Pair_ID'] == pair]\n",
    "    y_p_true = pair_data['Target_Direction'].values\n",
    "    \n",
    "    stats = {'Pair_ID': pair, 'Samples': len(pair_data)}\n",
    "    \n",
    "    for model in models:\n",
    "        pred_col = f\"{model}_Pred\"\n",
    "        y_p_pred = (pair_data[pred_col].values > 0.5).astype(int)\n",
    "        \n",
    "        # We focus on Accuracy and Precision for per-pair selection\n",
    "        acc = accuracy_score(y_p_true, y_p_pred)\n",
    "        prec = precision_score(y_p_true, y_p_pred, zero_division=0)\n",
    "        \n",
    "        stats[f'{model}_Acc'] = acc\n",
    "        stats[f'{model}_Prec'] = prec\n",
    "        \n",
    "    pair_stats.append(stats)\n",
    "\n",
    "pair_df = pd.DataFrame(pair_stats).set_index('Pair_ID')\n",
    "pair_df = pair_df.sort_values(by='LSTM_Prec', ascending=False)\n",
    "\n",
    "# Display Top 5 Pairs\n",
    "print(\"TOP 5 PAIRS (by LSTM Precision):\")\n",
    "print(pair_df[['Samples', 'Ridge_Prec', 'LSTM_Prec']].head(5).round(4).to_string())\n",
    "\n",
    "print(\"\\nBOTTOM 5 PAIRS (Avoid these):\")\n",
    "print(pair_df[['Samples', 'Ridge_Prec', 'LSTM_Prec']].tail(5).round(4).to_string())\n",
    "\n",
    "\n",
    "# 4. MODEL AGREEMENT & HYBRID SIGNAL\n",
    "print(\"MODEL CONSENSUS ANALYSIS\")\n",
    "\n",
    "\n",
    "# Create a \"Hybrid\" signal: Trade ONLY if BOTH models say YES (1)\n",
    "df['Hybrid_Pred'] = ((df['Ridge_Pred'] == 1) & (df['LSTM_Pred'] == 1)).astype(int)\n",
    "\n",
    "hybrid_acc = accuracy_score(y_true_binary, df['Hybrid_Pred'])\n",
    "hybrid_prec = precision_score(y_true_binary, df['Hybrid_Pred'], zero_division=0)\n",
    "trade_count = df['Hybrid_Pred'].sum()\n",
    "\n",
    "print(f\"Strategy: Trade only when BOTH models agree (Converge)\")\n",
    "print(f\"   - Trades Triggered: {trade_count} / {len(df)}\")\n",
    "print(f\"   - Hybrid Accuracy:  {hybrid_acc:.4f}\")\n",
    "print(f\"   - Hybrid Precision: {hybrid_prec:.4f}\")\n",
    "\n",
    "if hybrid_prec > metrics_df.loc['LSTM', 'Precision']:\n",
    "    print(\"\\n INSIGHT: The Hybrid consensus improves Precision! Use this for safer trades.\")\n",
    "else:\n",
    "    print(\"\\n INSIGHT: Hybrid does not improve Precision. Stick to the single best model.\")\n",
    "\n",
    "\n",
    "\n",
    "# 5. SAVE REPORTS\n",
    "\n",
    "output_metrics = '../data/processed/06_global_metrics.csv'\n",
    "output_pairs = '../data/processed/06_pair_performance.csv'\n",
    "\n",
    "metrics_df.to_csv(output_metrics)\n",
    "pair_df.to_csv(output_pairs)\n",
    "\n",
    "print(f\"Saved global metrics to: {output_metrics}\")\n",
    "print(f\"Saved pair performance to: {output_pairs}\")\n",
    "print(\"\\n Evaluation Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "588cc853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table successfully saved to: ../results\\models_metrics.csv\n",
      "Table successfully saved to: ../results\\pair_performance.csv\n"
     ]
    }
   ],
   "source": [
    "#save the models performance metrics and pair perfomances to outputs folder\n",
    "import os\n",
    "\n",
    "# 1. Define the paths\n",
    "source_path = '../data/processed/06_model_metrics.csv'\n",
    "output_dir = '../results'\n",
    "output_path = os.path.join(output_dir, 'models_metrics.csv')\n",
    "\n",
    "# 2. Load the CSV \n",
    "df_final = pd.read_csv(source_path)\n",
    "\n",
    "# 4. Save the table to the output folder\n",
    "df_final.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Table successfully saved to: {output_path}\")\n",
    "\n",
    "source_path = '../data/processed/06_pair_performance.csv'\n",
    "output_dir = '../results'\n",
    "output_path = os.path.join(output_dir, 'pair_performance.csv')\n",
    "df_final = pd.read_csv(source_path)\n",
    "# 4. Save the table to the output folder\n",
    "df_final.to_csv(output_path, index=False)\n",
    "print(f\"Table successfully saved to: {output_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
